# ML CI/CD Pipeline - Created by Balaji Koneti
name: ML Model CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'ml/**'
      - '.github/workflows/ml-ci-cd.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'ml/**'
  schedule:
    # Run automated retraining every day at 2 AM UTC
    - cron: '0 2 * * *'

env:
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  MLFLOW_REGISTRY_URI: ${{ secrets.MLFLOW_REGISTRY_URI }}

jobs:
  # Job 1: Code Quality and Testing
  code-quality:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('ml/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      working-directory: ./ml
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov black flake8 mypy
        
    - name: Code formatting check
      working-directory: ./ml
      run: black --check --diff src/ tests/
      
    - name: Linting
      working-directory: ./ml
      run: flake8 src/ tests/ --max-line-length=100 --ignore=E203,W503
      
    - name: Type checking
      working-directory: ./ml
      run: mypy src/ --ignore-missing-imports
      
    - name: Run tests
      working-directory: ./ml
      run: |
        pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
        
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./ml/coverage.xml
        flags: ml
        name: ml-coverage

  # Job 2: Data Validation
  data-validation:
    runs-on: ubuntu-latest
    needs: code-quality
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      working-directory: ./ml
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run data validation
      working-directory: ./ml
      run: |
        python -m src.data_pipeline.validate_data --config config/data_validation.yml
        
    - name: Upload validation reports
      uses: actions/upload-artifact@v3
      with:
        name: data-validation-reports
        path: ml/reports/validation/

  # Job 3: Model Training and Evaluation
  model-training:
    runs-on: ubuntu-latest
    needs: [code-quality, data-validation]
    if: github.event_name == 'push' || github.event_name == 'schedule'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      working-directory: ./ml
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Set up MLflow
      run: |
        # Start MLflow server in background
        mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 &
        sleep 10
        
    - name: Run model training
      working-directory: ./ml
      env:
        MLFLOW_TRACKING_URI: http://localhost:5000
      run: |
        python -m src.train --config config/training_config.yml --experiment-name "ci-cd-training"
        
    - name: Run model evaluation
      working-directory: ./ml
      env:
        MLFLOW_TRACKING_URI: http://localhost:5000
      run: |
        python -m src.evaluate --model-version latest --test-data data/test/test_data.csv
        
    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: model-artifacts
        path: |
          ml/models/
          ml/reports/
          ml/notebooks/

  # Job 4: Model Performance Validation
  model-validation:
    runs-on: ubuntu-latest
    needs: model-training
    if: github.event_name == 'push' || github.event_name == 'schedule'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      working-directory: ./ml
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: model-artifacts
        path: ml/
        
    - name: Validate model performance
      working-directory: ./ml
      run: |
        python -c "
        import json
        import sys
        
        # Load evaluation results
        with open('reports/evaluation_results.json', 'r') as f:
            results = json.load(f)
        
        # Check performance thresholds
        f1_score = results['metrics']['f1_score']
        recall = results['metrics']['recall']
        accuracy = results['metrics']['accuracy']
        
        print(f'Model Performance:')
        print(f'  F1 Score: {f1_score:.3f}')
        print(f'  Recall: {recall:.3f}')
        print(f'  Accuracy: {accuracy:.3f}')
        
        # Performance thresholds
        min_f1 = 0.80
        min_recall = 0.85
        min_accuracy = 0.75
        
        if f1_score < min_f1:
            print(f'❌ F1 score {f1_score:.3f} below threshold {min_f1}')
            sys.exit(1)
        if recall < min_recall:
            print(f'❌ Recall {recall:.3f} below threshold {min_recall}')
            sys.exit(1)
        if accuracy < min_accuracy:
            print(f'❌ Accuracy {accuracy:.3f} below threshold {min_accuracy}')
            sys.exit(1)
            
        print('✅ All performance thresholds met')
        "
        
    - name: Generate performance report
      working-directory: ./ml
      run: |
        python -c "
        import json
        from datetime import datetime
        
        # Load evaluation results
        with open('reports/evaluation_results.json', 'r') as f:
            results = json.load(f)
        
        # Generate performance report
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'model_version': results.get('model_version', 'unknown'),
            'performance_metrics': results['metrics'],
            'business_metrics': results.get('business_metrics', {}),
            'evaluation_summary': results.get('evaluation_summary', {}),
            'ci_cd_status': 'passed'
        }
        
        with open('reports/ci_cd_performance_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print('Performance report generated')
        "
        
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: ml/reports/ci_cd_performance_report.json

  # Job 5: Model Deployment
  model-deployment:
    runs-on: ubuntu-latest
    needs: model-validation
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      working-directory: ./ml
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: model-artifacts
        path: ml/
        
    - name: Download performance report
      uses: actions/download-artifact@v3
      with:
        name: performance-report
        path: ml/reports/
        
    - name: Deploy model to staging
      working-directory: ./ml
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        MLFLOW_REGISTRY_URI: ${{ secrets.MLFLOW_REGISTRY_URI }}
      run: |
        python -c "
        import mlflow
        import joblib
        import json
        from pathlib import Path
        
        # Load model
        model_path = Path('models/burnout_model.pkl')
        if model_path.exists():
            model = joblib.load(model_path)
            
            # Register model in MLflow
            with mlflow.start_run() as run:
                mlflow.sklearn.log_model(model, 'model')
                model_uri = f'runs:/{run.info.run_id}/model'
                
                # Register model
                registered_model = mlflow.register_model(
                    model_uri=model_uri,
                    name='burnout-risk-prediction'
                )
                
                # Transition to Staging
                client = mlflow.tracking.MlflowClient()
                client.transition_model_version_stage(
                    name='burnout-risk-prediction',
                    version=registered_model.version,
                    stage='Staging'
                )
                
                print(f'Model registered and deployed to Staging: v{registered_model.version}')
        else:
            print('No model found for deployment')
            exit(1)
        "
        
    - name: Run integration tests
      working-directory: ./ml
      run: |
        python -c "
        import requests
        import time
        
        # Wait for model to be available
        time.sleep(30)
        
        # Test model endpoint
        try:
            response = requests.post('http://localhost:8001/predict', 
                                   json={
                                       'user_id': 'test_user',
                                       'features': {
                                           'work_hours_per_week': 45,
                                           'stress_level': 7,
                                           'workload_score': 8
                                       }
                                   },
                                   timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                print(f'✅ Model prediction successful: {result[\"risk_level\"]}')
            else:
                print(f'❌ Model prediction failed: {response.status_code}')
                exit(1)
                
        except Exception as e:
            print(f'❌ Integration test failed: {str(e)}')
            exit(1)
        "
        
    - name: Deploy to production
      if: success()
      working-directory: ./ml
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        MLFLOW_REGISTRY_URI: ${{ secrets.MLFLOW_REGISTRY_URI }}
      run: |
        python -c "
        import mlflow
        
        # Get the latest staging model
        client = mlflow.tracking.MlflowClient()
        latest_versions = client.get_latest_versions('burnout-risk-prediction', stages=['Staging'])
        
        if latest_versions:
            version = latest_versions[0].version
            
            # Transition to Production
            client.transition_model_version_stage(
                name='burnout-risk-prediction',
                version=version,
                stage='Production'
            )
            
            print(f'✅ Model v{version} deployed to Production')
        else:
            print('❌ No staging model found for production deployment')
            exit(1)
        "
        
    - name: Notify deployment
      if: always()
      run: |
        if [ "${{ job.status }}" == "success" ]; then
          echo "✅ Model deployment successful"
        else
          echo "❌ Model deployment failed"
        fi

  # Job 6: Automated Retraining (Scheduled)
  automated-retraining:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      working-directory: ./ml
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run automated retraining
      working-directory: ./ml
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        MLFLOW_REGISTRY_URI: ${{ secrets.MLFLOW_REGISTRY_URI }}
      run: |
        python -c "
        import asyncio
        from src.mlops.automated_retraining import AutomatedRetrainingPipeline
        
        async def main():
            config = {
                'retraining_triggers': {
                    'time_interval_days': 1,  # Daily for CI/CD testing
                    'performance_threshold': 0.1,
                    'drift_threshold': 0.15,
                    'data_size_threshold': 100
                },
                'deployment': {
                    'auto_deploy': False,  # Manual approval required
                    'performance_threshold': 0.80,
                    'recall_threshold': 0.85
                },
                'models_dir': 'models',
                'data_dir': 'data',
                'backup_dir': 'backups'
            }
            
            pipeline = AutomatedRetrainingPipeline(config)
            await pipeline.initialize()
            
            result = await pipeline.run_monitoring_cycle()
            print(f'Automated retraining result: {result}')
        
        asyncio.run(main())
        "
        
    - name: Upload retraining results
      uses: actions/upload-artifact@v3
      with:
        name: retraining-results
        path: |
          ml/models/
          ml/backups/
          ml/reports/

